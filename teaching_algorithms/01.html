<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Intro to Algorithms and Data Structures</title>
  <style>
    /* html, body {
      width: 100%;
    } */
    canvas {
      display: block;
      outline: 1px solid black;
      margin:auto;
      /* width: 50%; */
    }
  </style>
  <script src="../util/math.js"></script>
  <script src="find_missing_number_matrix_game.js"></script>
  <script src="big_o_game.js"></script>
  <script>
    function main() {
      startGame()
    }
  </script>
</head>
<body onload="main()">
  <div>
<h1>What are algorithms?</h1>
  Below is a brain training game I learned about when I was helping out at an
  elder care center. The game consists of a grid with 9 rows and 10 columns, with
  numbers from 1 to 100. 10 numbers are missing, and the goal of the game is to
  write the ten missing numbers in the empty cells of the bottom row. Try to take
  some time to find at least one missing number, and write down instructions of
  how a resident at such an elder care center could solve the game. (Feel free to
  find all the missing numbers if you want.)

  <br>
  <canvas id="canvas" width="1200" height="540"></canvas> <!-- Add an explanation to the game. "Click me" -> "Write" -> "Delete" -->
  <br>

  <span style="background-color: grey;font-weight: bold;">So what does any of this have to do with algorithms? </span>

  Simply put, an algorithm is a series of instructions to solve a
  problem(FOOTNOTE: How the term "algorithm" is used online and on TV). If you
  wrote down instructions to follow in order to solve the game above, then you
  made an algorithm. One useful way to think of algorithms is the following
  figure. Given some input to the algorithm, the algorithm gives some output.
  Taking the above game as an example, the 9 by 10 grid of numbers is the input,
  the algorithm is the instructions to follow to solve the game, and the output
  is the 10 missing numbers. Algorithms can also be compared to food recipies. A
  recipe takes some ingredients, and following the instructions of the recipe
  you (hopefully) get a delicious meal as output. 
  
  <br>
  <!-- Visual abstract representation of an algorithm -->
  <image>Input -> Black box (Algorithm) -> Output</image>

  <br>
  Knowing what an algorithm is, is probably the least interesting part of
  learning about algorithms. In the following sections we will go into more
  depth about about the more interesting parts of studying algorithms.

<h1>Why are algorithms?</h1>
    <!-- I am not really sure what to write here? How do I start this section? The
    other section jumped right into the actual meat of the matter. So maybe I 
    should do the same here? -->
    If you understand that algorithms are just a series of instructions to solve
    a problem, and you agree that the world has many problems to be solved, then
    you may not need more motivation about why algorithms are interesting.
    Still, I will go into some more details about the importance of study of
    algorithms.

    There are three important aspects of algorithms which were not mentioned in the 
    introduction. The first is rather obvious which is that we want our algorithms 
    to be correct. <b>Correctness</b> is the quality of an algorithm to not give
    wrong answers. This is probably the most important thing you could want,
    and people will go through great lengths to prove that their algorithm is correct,
    but in some situations (e.g. for some Randomized Algorithms) we allow algorithms 
    to give wrong answers in order to be faster.

    Another related property of algorithms is that they are <b>sound</b>, meaning that 
    the algorithm should not just give up on some problems. (Is this what soundness means?)

    The final important aspect is <b>growth</b> which may take a bit more effort
    to explain that the other aspects. Simply put, the growth of an algorithm is
    about how much slower an algorithm becomes as the size of the problem
    increases. 
    
    Taking the brain training game as an example, the algorithm that elderly
    resident used to solve it, was a very naive one. The algorithm goes as
    follows: Starting at the number 1, look through every cell in the matrix to
    see if you can find it anywhere, if you cannot, write it an empty cell in
    the bottom row. Do this for every number from 1 to 100 until every number is
    found.

    So why is this algorithm naive? It is both correct and sound (I would encourage 
    you to explain why), but the issue is that it is incredibly slow. If you try 
    following this on the 9 by 10 grid, you can probably finish it in a reasonable 
    amount of time. It may feel like it takes a bit of time, but nothing unmanagable. 
    But try imagining a 29 by 30 grid instead. There are only around 9 times more numbers 
    to deal with, but you may think the problem feels a lot more time consuming than just 
    solving 9 additional, 9 by 10 grid. Which it does, because it is. This is one of the key observations 
    about growth.   (This kind of "naive", but correct algorithm, is often
    called a brute force algorithm. What is meant by brute force is that the
    problem is solved by computation power rather than any clever techniques. 
    A brute force algorithm often just tries everything untill it finds the correct 
    solution, which makes them easy to think of and implement, and also good to test 
    against a more sophisticated algorithm which may have non-obvious errors.)

    The issue with the naive algorithm is that for every number we check, we
    might have to look through most, or all the numbers in the grid in order to
    determine if it the number is there or not. This means that we may look at
    each of the 90 cells in the grid, close to 100 times. This is known as a 
    quadratic growth, because the time spend is proportional to the number of 
    cells in the grid, times the number of numbers to check. That is, even
    though there are only 90 cells in the grid, we may have looked at check
    close to 9000 cells in total when we are finished with our algorithm. You
    can argue that a lot of the times you do not look through all 90 cells in
    the grid before you find the number you are looking at, but even if you
    found every number in around half, or maybe even only one fourth of the 90
    cells, we are still looking at thousands of cells checked. Now imagine what 
    happens when there are 9 times more numbers? Suddenly, the problem becomes
    close to 100 times worse. 

    This is where the most fun part of algorithm comes in. Because now that we 
    understand the problem with the naive algorithm, we can ask ourselves, "Can
    we do better?". And the answer to that question is, "Yes, and by a lot". I would 
    encourage you to try to come up with a faster algorithm before reading further on. 
    When you are ready, continue to the final part of this section. 

    <!--INSERT HIDE SPOILER BLOCK. NOTE: Let javascript hide it such that if somebody has javascript disabled they don't notice -->
    So how can we make a better algorithm to solve the problem? 

    The issue is that we keep having to look at the same cells many times in
    order to find the missing numbers, but what if instead of looking after the
    missing numbers, we instead make sense of the numbers we have? 

    Instead of checking every number to see if it is there, it would be more
    efficient if we could organize the existing numbers in a way which made it
    clear which numbers were missing. 

    To do this, we can create another 10 by 10 grid, with the numbers from 1 to
    100 in sorted order. Now, we can just look at each cell in our 9 by 10 grid
    once, and cross it out in our 10 by 10 grid. When we are finished with every
    cell in the 9 by 10 grid, we look through the 10 by 10 grid to see which
    numbers are not crossed out. These are the 10 missing numbers. 
    
    So, how much faster is this new method?

    We can split the time spent on this new algorithm into three parts. (1)
    looking at cells in the 9 by 10 grid, (2) crossing cells out in the 10 by 10
    grid, and (3) looking through the 10 by 10 grid to find the missing numbers.
    Notice how (1) and (3) only requires looking at each cell once because we
    can just scan over the numbers, a remarkable improvement over the naive
    algorithm. (2) is a bit harder to analyse because finding the right cell in
    the 10 by 10 grid to cross out is not instant but since the numbers are
    sorted it is a lot faster. There is a number of ways we could do this, but
    for simplicity, we will assume that we actually can find it instantly. This 
    may seem like cheating (and it is), but it is very close to reality if a 
    computer were to do it, because a computer can calculate the position of the
    cell and look exactly where it is. {FOOTNOTE: The only minor error we aren't
    taking into consideration for a computer has to do with cache and
    cache-misses, but this is outside the scope of an introduction. } Given this
    simplification, we can see that we at most look at each number from 1 to
    100, three times. Two times to look at the number in the 9 by 10 grid and
    cross it out in the 10 by 10 grid, and one extra time to find the missing
    numbers. 

    Let us compare. The naive algorithm had a quadratic growth, meaning that the
    number of times we looked at each number in the grid grew the larger the
    grid was. In contrast, this new algorithm only requires looking at each
    number at most three times. This is known as a linear growth, meaning that 
    if the size of the problem doubles, so does the time to solve it, whereas 
    for the quadratic algorith, if the problem doubled, the time to solve it
    quadrubles. 

    [TODO: INSERT GRAPH]

    EXERCISE: 
    [One thing is to read an explanation, but it better if you can experience it 
    for yourself. Try to solve the grid problem using the naive and the faster
    method. You can also try to increase the size of the problem to feel how 
    much slower the naive algorithm becomes when the size increases. 

    [TODO: Insert grid with toggable help, with resizing, and reset]

    ]
    
    EXERCISE: [
    If you know how to program, you can try to write a program which 
    solves the grid problem using the naive technique and the faster technique. 
    Try to compare how long it takes to solve the problem for the two algorithms 
    on different sized grids. 
    ]
    
    The key takeaway of this section is that the difference between a fast and a
    slow algorithm is not just that one takes "longer" than the other, but
    rather that one <b>grows</b> faster than another. When designing algorithms,
    the goal is (most often) to design an algorithm which grows the slowest,
    meaning that even if a problem size is hundreds of billions elements, the
    algorithm minimizes how many times it needs to look at each, with the ideal
    being not even looking at an element unless it is absolutely required. 

    In future sections we will see example of algorithms with different growth
    rate. In the next section we will cover some of the notation and techniques
    used when analyzing algorithm.
    
<!-- Correctness, soundness, growth,  -->

<!-- To explain growth, go back to the brain training game and compare the naive solution to the smart solution.  -->

<!-- The idea that some solutions are not just slower, but also becomes even worse as the input increases is a key takeaway. -->


<h1>How are algorithms?</h1>
    As the final part of this introduction, we will go into some details about 
    how algorithms and their performance are described and analyzed in general. 
    This will involve some simple (but maybe confusing) mathematical modeling.

    <!-- This section is quite shit I feel like -->
    <!-- Maybe just make some graphs and point out it out from those -->
    Let us first consider an issue we have when comparing the performance of
    algorithm. We can describe the time it takes to run an algorithm on some
    input size $n$, using some function $f(n)$. So for the naive algorithm for
    the grid problem, a function which describes the time time it takes could be
    something like $f(n) = (1/2)n^2 + 5n + 104$, don't think too deeply about
    the choice of constants, the point is that it might be best described as
    some kind of polynomial function. Now, there isn't really any issues with
    this per say, but we might notice that already when $n$ becomes 15, the
    biggest term of the polynomial is the term including the $n^2$, and it will
    continue to be so. So the terms $5n$ and $104$ aren't very important to 
    understand the growth of the algorithm. If we just know that the biggest 
    term is $(1/2)n^2$, then we already have a solid idea of how fast the 
    algorithm is. Furhermore, if we consider another algorithm with a running 
    time described as $g(n) = 3n^2 + 50n + 1$, we can again notice the biggest 
    term is $3n^2$, but furthermore, we can see that $f(n)$ and $g(n)$, has 
    the same growth, since both are quadratic. This is a bit unclear due to 
    all the factors and constants, which is why the <b>Big O</b> notation exists.

    
    The Big O notation is used to describe the growth of an algorithm, in terms
    of a "family" of functions. The idea behind the notation, is that if we only
    care about the "growth" of an algorithm, then we can ignore constants and
    slower growing terms. The Big O notation gets its name from the fact that we
    write $f(n) \in O(h(n))$ to say that the function $f(n)$ is in the family of
    functions with growth described by $h(n)$. To give some examples, our naive
    algorithm for the grid problem has growth $O(n^2)$, and our fast algorithm
    has growth $O(n)$. I.e., whatever function $f(n)$ which describes the
    running time of the naive algorithm is in the family of functions $O(n^2)$. 
    Here, $O(n^2)$ essentially means any function where $n^2$ is the biggest term, 
    and whatever factor $n^2$ may be multiplied with is ignored.  
    

    <!-- TODO: Big O tests -->
    <!-- What kind of test? 
    1. Simple Yes/No questions of whether f(n) \in O(g(n))  
    2. Ordering some functions 
    3. More tricky niche stuff like: Is there some $k$ such that log(n)^k is not O(n)?
    4. Match a graph curve to a function
    5. More advanced. Look at code and give Big O?
    6. Simplify an expression to the simplest Big O (Few terms, no constants)
    7. Read a description and give a Big O? (E.g. looks at half a list 3 times to calculate X, what is Big O?)
   
    A lot of understanding, not much applied. How can we apply?
    -->

    <!-- 
    This seems like a lot of work. It also seems kind of fun. If it seems fun, then why not just do it?
      How do we internally model the relationship between functions? We first need to model the functions and then we can simply sort them. 
      How do we model functions? 
      What functions do we want?
      1. constant
      2. logarithmic + constant
      3. logarithmic^k + logarithmic^{l} + logarithmic^{m} + ... + constant (where l < k, m < k, ...)
      4. linear
      5. linearithmic
      6. polynomial (quadratic, qubic, etc.)
      7. exponential
      8. factorial
    -->

<!-- Big O notation? -->
<!-- Math -->
<!-- Proofs -->

<h1>Notes and thoughts (TO BE DELETED)</h1>
So what are the learning outcomes of reading this?

General goal of understanding what algorithms are.
This is essentially a side goal, since understanding what algorithms are best comes from seeing a lot of algorithms (Ithink?).
<!-- Does it even make sense to understand the concept of algorithms without understanding algorithms? I do not really think so.  -->

So to understand algorithms, I want to show algorithms to people. 

Algorithms can be on various degrees of specificity. 
Quicksort can be explained in a single sentence, or it can be described in like 100 lines of code. 

My goal is to have people understand the grand picture instead of the nitty gritty. 
I want to build intuition and directly applicable skills, rather than learning how the partition of quicksort works. 
Going into the detail of the partition step may be useful to train some set of skills, but I am not really sure what those skills are. 
Until I understand the reason to go into detail, I don't think it is worth doing that. 

I want people to be able to describe algorithms in different levels of specificity.  

Very general broad descriptions, down to very nitty gritty taking care of all edge cases levels. 
The nitty gritty is important if you want to actually implement algorithms. So maybe nitty gritty is  
left as a bonus? But on the other hand. If you don't have the skills to create it, do you really understand it? 
Maybe it is possible to have a good understanding, but not the coding skills to program it. 
Can we somehow test for the algorithmic skills without the coding skills? Maybe following the steps of the algorithm is enough?


The 


Core principles for learning.
1. Visual learning (related to 3)
2. Doing instead of just reading
3. Building upon existing knowledge
4. Reflection (related to 2)
5. Women seem to be more engaged by problems involving people
6. 


  </div>
</body>
</html>